********************************SET-UP********************************
sudo apt update
sudo apt install openjdk-17-jdk -y
sudo apt install openjdk-17-jdk -y
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 -> SET JAVA HOME

wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
tar -xvzf spark-3.5.5-bin-hadoop3.tgz

sudo apt update
sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev \
    libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev

wget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tgz
tar -xf Python-3.9.1.tgz
cd Python-3.9.1
./configure --enable-optimizations
make -j $(nproc)
sudo make altinstall
python3.9 --version

source venv/bin/activate

**Uploaded searchlog.csv to spark_project folder where python file is also located



********************************Pre-Process FILE********************************

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import re

# Initialize Spark session
spark = SparkSession.builder.appName("SearchLogParser").getOrCreate()

# Read the log file as a text file
lines = spark.read.text("searchLog.csv").rdd.map(lambda r: r[0])

def parse_line(line):
    m = re.match(r"searchTerm:\s*[â€˜']?(.*?)[â€™']?,\s*(.*)", line)
    if not m:
        return []
    term = m.group(1).strip()
    rest = m.group(2).strip()
    # split by '~' to get url:click pairs
    url_clicks = rest.split("~")
    records = []
    for uc in url_clicks:
        # Remove any trailing '>' or whitespace
        uc = uc.strip().rstrip('>')
        if ':' not in uc:
            continue
        url, clicks = uc.split(":", 1)
        url = url.strip()
        try:
            clicks = int(re.match(r"\d+", clicks.strip()).group())
        except Exception:
            continue
        records.append( (term, url, clicks) )
    return records

# FlatMap to get all (term, url, clicks) tuples
parsed_records = lines.flatMap(parse_line).collect()

schema = StructType([
    StructField("term", StringType(), True),
    StructField("url", StringType(), True),
    StructField("clicks", IntegerType(), True)
])

# Create DataFrame from list of tuples
df = spark.createDataFrame(parsed_records, schema)

# Write as JSON, one record per file (coalesce to 1 per file)
df.repartition(df.count()).write.mode("overwrite").json("processed_data")

spark.stop()



********************************QUERY FILE********************************


from flask import Flask, request, jsonify
import glob
import json
import os
from collections import OrderedDict, defaultdict
import tldextract


app = Flask(__name__)
app.config["JSON_SORT_KEYS"] = False


def domain_priority(url):
    ext = tldextract.extract(url)
    tld = ext.suffix
    if tld == 'org':
        return 0
    elif tld == 'edu':
        return 1
    elif tld == 'com':
        return 2
    else:
        return 3


# Load all json files into a list of records
def load_data():
    data = []
    for filename in glob.glob('processed_data/part-*.json'):
        with open(filename, 'r') as f:
            for line in f:
                data.append(json.loads(line))
    return data


# Cache the data for efficiency
DATA = load_data()


@app.route('/results', methods=['POST'])
def results():
    req = request.get_json()
    term = req.get('term')
    
    # Aggregate clicks per URL for the search term
    url_clicks = {}
    for record in DATA:
        if record['term'] == term:
            url = record['url']
            clicks = int(record['clicks'])
            url_clicks[url] = url_clicks.get(url, 0) + clicks

    sorted_items = sorted(
        url_clicks.items(),
        key=lambda x: (-x[1], domain_priority(x[0]))
    )

    results = {url: clicks for url, clicks in sorted_items}
    results_final = OrderedDict(sorted_items)
   
    return app.response_class(
        response=json.dumps({"results": results_final}),
        status=200,
        mimetype='application/json'
    )


@app.route('/trends', methods=['POST'])
def trends():
    req = request.get_json()
    term = req.get('term')

    total_clicks = 0
    for record in DATA:
        if record['term'] == term:
            total_clicks += int(record['clicks'])
    return jsonify({"clicks": total_clicks})


@app.route('/popularity', methods=['POST'])
def popularity():
    req = request.get_json()
    url = req.get('url')

    total_clicks = 0
    for record in DATA:
        if record['url'] == url:
            total_clicks += int(record['clicks'])
    return jsonify({"clicks": total_clicks})


@app.route('/getBestTerms', methods=['POST'])
def get_best_terms():
    req = request.get_json()
    website = req.get('website')
    
    # Aggregate clicks per term for the given website
    term_clicks = defaultdict(int)
    total_clicks = 0

    for record in DATA:
        if record['url'] == website:
            clicks = int(record['clicks'])
            term_clicks[record['term']] += clicks
            total_clicks += clicks

    if total_clicks == 0:
        return jsonify({"best_terms": []})

    best_terms = []
    for term, clicks in term_clicks.items():
        proportion = clicks / total_clicks
        if proportion > 0.05:
            best_terms.append(term)
    return jsonify({"best_terms": best_terms})


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
